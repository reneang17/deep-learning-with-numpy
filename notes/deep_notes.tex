\documentclass[a4paper,11pt]{article}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage{mathrsfs}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{bbm}
\usepackage{dsfont}
\usepackage{braket}



\newcommand{\dd}{\text{d}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bJt}{\mathbf{J}^{(0)}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\mc}{\mathcal{M}}
\newcommand{\bj}{\bar{j}}
\newcommand{\pola}{\varepsilon}
\newcommand{\ldot}{\!\cdot\!}


\newcommand{\na}[2]{a_{#1}^{[#2]}}
\newcommand{\nb}[2]{b_{#1}^{[#2]}}
\newcommand{\nz}[2]{z_{#1}^{[#2]}}

\newcommand{\na}[3]{a_{#1 #2}^{[#2]}}

\newcommand{\nz}[3]{z_{#1 #2}^{[#2]}}


\newcommand{\nt}[3]{\theta_{#1 #2}^{[#3]}}

\newcommand{\nt}[4]{\theta_{#1 #2 #3}^{[#4]}}

\newcommand{\sg}{ \sigma_{\text{sigmoid}}}

\newcommand{\ff}[2]{#1 \left(#2 \right)}


\newcommand{\blue}[1]{\textcolor{blue}{#1}}
%\newcommand{\sla}[1]{\slashed{#1}}


%\textwidth = 460pt
%\textheight = 560pt
%\oddsidemargin =17pt

\title{Feedforward Neural Network}
%\author{R. \'Angeles-Mart\'inez }
%
%
%\emailAdd{reneang17@gmail.com}


%\abstract{All orders loop evolution and its large $N$ limit.}
%% \keywords{Renormalization, electromagnetic properties.}
%% \pacs{11.10.Gh,11.30.Cp,13.40.Em}
%%\tableofcontents
%
%
\begin{document}



In the lectures notes, one has $\theta^{[l]}_{\text{here}}=\theta^{[l-1]}_{\text{there}}$, where
there refers to the machine learning learning course by Andrew.
We do this change to make the connexion with the
deep learning specialization lectures.


\section{Structure of dense layers}

%In the lectures notes, one has
%$\theta^{[l]}_{\text{here}}=\theta^{[l-1]}_{\text{there}}$.
%We do this change to make the connexion with the
%deep learning specialization lectures.

\begin{equation}
    \na{\mu}{0} \equiv \delta_{0\mu}+x_i\delta_{i\mu}  %\label{eq:}
\end{equation}

\begin{equation}
    \na{\mu}{l} \equiv \delta_{0\mu} +
    \ff{g^{[l]}}{ \na{\nu }{l-1} \nt{\nu}{i}{l}  }\,\, \delta_{i\mu}^{[l]}  %\label{eq:}
\end{equation}

\begin{equation}
    \nz{i}{l}\equiv\na{\nu }{l-1}  \nt{\nu}{i}{l} %\label{eq:}
\end{equation}

It should be understood that in $\na{i}{l}$
one has  $i \in \{1,..., s_l\} $,
and that in $\na{\mu}{l}$ one has $ \mu \in \{0,..., s_l\}$. The binary cross entropy cross function is

\begin{equation}
    cost=\frac1m \sum^m_n
    J \Big\vert_{x=x_n,y=y_n}  %\label{eq:}
\end{equation}
where

\begin{equation}
    J\equiv -\sum^{s_L}_k
    \left(
        y_k\ff{\log}{\na{k}{L}}  %\label{eq:}
        +(1-y_k)\ff{\log}{1-\na{k}{L}}
    \right)
\end{equation}





\section{Backward propagation.  Recursion relation}

The following recursion relations do the job
\begin{equation}
    \underbrace{
    \frac{\partial J }{\partial  \nt{j_{l-1}}{j_{l}}{l}  }
    }_{\dd  \nt{j_{l-1}}{j_l}{l} }
   =
   \underbrace{
    \frac{\partial \nz{j_l}{l} }{\partial \nt{j_{l-1}}{j_l}{l}  }
    }_{ \na{j_{l-1}}{l-1} }
   \underbrace{
    \frac{\partial J  }{\partial \nz{j_l}{l} }
    }_{\dd  \nz{j_{l}}{l} }
    %\label{eq:}
\end{equation}



\begin{equation}
\underbrace{
\frac{\partial J  }{\partial \nz{j_l}{l}   }
}_{\dd  \nz{j_{l}}{l} }
	=
	\underbrace{
	\frac{\partial J  }{\partial \nz{j_{l+1}}{l+1}   }
	\frac{\partial \nz{j_{l+1}}{l+1}}{\partial \na{j_{l}}{l}}
	}_{\dd a_{j_l}^{[l]} }
	\frac{\partial \na{j_{l}}{l}    }{ \partial \nz{j_{l}}{l}  }
	=
	\underbrace{
	\frac{\partial J  }{\partial \nz{j_{l+1}}{l+1}   }
	 \nt{j_l}{j_{l+1}}{l+1}
	 }_{ \dd a_{j_l}^{[l]}=\theta^{[l+1]}_{j_l j_{l+1}} \dd z_{j_{l+1}}^{[l+1]} }
	 g^{[l]'}\left( z_{j_{l}}^{[l]}   \right)
\end{equation}

Hence, in an abbreviated notation one has:
\begin{eqnarray}
	&\dd  \nt{j_{l-1}}{j_l}{l} = \dd  \nz{j_{l}}{l}  \na{j_{l-1}}{l-1}\\
	&\dd a_{j_{l}}^{[l]}=\theta^{[l+1]}_{j_{l} j_{l+1}} \dd z_{j_{l+1}}^{[l+1]}\\
	&\dd  \nz{j_{l}}{l}= \dd a_{j_l}^{[l]} \,\, g^{[l]'}\left( z_{j_{l}}^{[l]}   \right)
\end{eqnarray}

The initial condition is


\begin{eqnarray}
    &\dd \theta^{[L]}_{\mu j_L}= \frac{\partial J }{\partial  \nt{\mu}{j_L}{L}  } =
    \underbrace{
    \frac{(\na{j_L}{L} -y_{j_L})}{\na{j_L}{L} (1-\na{j_L}{L-1} )}
    }_{\dd a_{j_{L}}^{[L]} }
    \,\, g^{[L]'} \left( z_{j_{L}}^{[L]} \right)
   \, \na{\mu}{L-1} \\
   & \dd z^{[L]}_{j_L }= \dd a_{j_{L}}^{[L]}\,
   g^{[L]'} \left( z_{j_{L}}^{[L]} \right)\\
   &\dd \theta^{[L]}_{j_L \mu}=  \dd \nz{j_L }{L}\, \na{\mu}{L-1}
\end{eqnarray}



This layer actually simplifies because one chooses $g^{[L]}= \sg$,
which means that $\sg'= \sg (1- \sg) $.



\section{Propagation with softmax}


\begin{equation}
    \na{\mu}{0} \equiv \delta_{0\mu}+ x_i\delta_{i\mu}  %\label{eq:}
\end{equation}

\begin{equation}
    \na{\mu}{l} \equiv \delta_{\mu0} +\delta_{\mu i }^{[l]}\,\,
    \ff{g^{[l]}}{\na{\nu }{l-1} \nt{\nu}{i}{l}  } , \,\,\,\, l<L %\label{eq:} ,
\end{equation}

\begin{equation}
    \nz{i}{l}\equiv \na{\nu }{l-1} \nt{\nu}{i}{l} ,\,\,\,\, l\le L %\label{eq:}
\end{equation}

\begin{align}
 \hat{y}_i \equiv   \frac{e^{\nz{i}{L}} } {\sum_c e^{\nz{c}{L} }}
\end{align}

It should be understood that in $\na{i}{l}$
one has  $i \in \{1,..., s_l\} $,
and that in $\na{\mu}{l}$ one has $ \mu \in \{0,..., s_l\}$. The loss function is

\begin{equation}
    J\equiv -\sum_i y_i \ff{\log}{\hat{y}_i }
\end{equation}

\section{Back propagation}

The following recursion relations do the job
\begin{equation}
    \underbrace{
    \frac{\partial J }{\partial  \nt{j_{l-1}}{j_{l}}{l}  }
    }_{\dd  \nt{j_{l-1}}{j_l}{l} }
   =
   \underbrace{
    \frac{\partial \nz{j_l}{l} }{\partial \nt{j_{l-1}}{j_l}{l}  }
    }_{ \na{j_{l-1}}{l-1} }
   \underbrace{
    \frac{\partial J  }{\partial \nz{j_l}{l} }
    }_{\dd  \nz{j_{l}}{l} }
    %\label{eq:}
\end{equation}



\begin{equation}
\underbrace{
\frac{\partial J  }{\partial \nz{j_l}{l}   }
}_{\dd  \nz{j_{l}}{l} }
	=
	\underbrace{
	\frac{\partial J  }{\partial \nz{j_{l+1}}{l+1}   }
	\frac{\partial \nz{j_{l+1}}{l+1}}{\partial \na{j_{l}}{l}}
	}_{\dd a_{j_l}^{[l]} }
	\frac{\partial \na{j_{l}}{l}    }{ \partial \nz{j_{l}}{l}  }
	=
	\underbrace{
	\frac{\partial J  }{\partial \nz{j_{l+1}}{l+1}   }
	 \nt{j_l}{j_{l+1}}{l+1}
	 }_{ \dd a_{j_l}^{[l]}=\theta^{[l+1]}_{j_l j_{l+1}} \dd z_{j_{l+1}}^{[l+1]} }
	 g^{[l]'}\left( z_{j_{l}}^{[l]}   \right)
\end{equation}

Hence, in an abbreviated notation one has:
\begin{eqnarray}
	&\dd  \nt{j_{l-1}}{j_l}{l} = \dd  \nz{j_{l}}{l}  \na{j_{l-1}}{l-1}\\
	&\dd a_{j_{l}}^{[l]}=\theta^{[l+1]}_{j_{l} j_{l+1}} \dd z_{j_{l+1}}^{[l+1]}\\
	&\dd  \nz{j_{l}}{l}= \dd a_{j_l}^{[l]} \,\, g^{[l]'}\left( z_{j_{l}}^{[l]}   \right)
\end{eqnarray}

The initial condition is ($\hat{y_i} = g(z_i)$)
\begin{eqnarray}
    &\dd \theta^{[L]}_{ j_{L-1} j_L}= \frac{\partial J }{\partial  \nt{j_{L-1}}{j_{L}}{L}  } =
   \left( -  \frac{y_i }{ \hat{y}_i} \right)
   \left(  \frac{ \partial \hat{y}_i}{\partial z_{j_L}^{[L]} }    \right)
   \left(   \frac{ \partial z_{j_L}^{[L]} } {\partial  \nt{j_{L-1}}{j_L}{L}  }  \right)
   \label{eq:softmaxp1}
\end{eqnarray}

\begin{align}
 \frac{ \partial \hat{y}_i}{\partial z_{j_L}^{[L]} } =\hat{y}_i(  \delta_{i j_L} -   \hat{y}_{j_L})
 \label{eq:softmaxp2}
\end{align}
\begin{align}
\frac{ \partial z_{j_L}^{[L]} } {\partial  \nt{j_{L-1}}{j_L}{L}  }  =  a^{L-1}_{j_{L-1}}
\label{eq:softmaxp3}
\end{align}
Note that after summing over $i$, the first two terms in \eqref{eq:softmaxp1} simplify as:
\begin{eqnarray}
   \left( -  \frac{y_i }{ \hat{y}_i} \right) \left(  \frac{ \partial \hat{y}_i}{\partial z_{j_L}^{[L]} }    \right) = - y_i + \hat{y}_i
\end{eqnarray}


*******************************************************




\end{document}
