{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e21a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eef275",
   "metadata": {},
   "source": [
    "## Neural network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c65881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    loss: (class) Loss function\n",
    "    validation: (tuple) Validation data (X, y)\n",
    "    \"\"\"\n",
    "    def __init__(self, Loss_function, validation_data=None):\n",
    "        self.layers = []\n",
    "        self.loss_function = Loss_function()\n",
    "        self.errors = {\"training\": [], \"validation\": []}\n",
    "        self.val_set = None\n",
    "        if validation_data:\n",
    "            X, y = validation_data\n",
    "            self.val_set = {\"X\": X, \"y\": y}\n",
    "\n",
    "    def add(self, layer):\n",
    "        \"\"\" Method which adds a layer to the neural network \"\"\"\n",
    "        # If not first layer, curent layer input = previous layer output \n",
    "        if self.layers:\n",
    "            layer.set_input_shape(shape=self.layers[-1].get_output_shape())\n",
    "        if hasattr(layer, 'initialize'):\n",
    "            layer.initialize()\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def train_on_batch(self, X, y):\n",
    "        \"\"\" Forward/backward on batch \"\"\"\n",
    "        #fordward prop\n",
    "        y_pred = self._forward(X)\n",
    "        loss = np.mean(self.loss_function.loss(y, y_pred))#(1) mean normalizes\n",
    "        acc = self.loss_function.acc(y, y_pred)\n",
    "        # backward prop\n",
    "        loss_grad = self.loss_function.gradient(y, y_pred)\n",
    "        self._backward(loss_grad=loss_grad)\n",
    "        return loss, acc\n",
    "\n",
    "    def test_on_batch(self, X, y):\n",
    "        \"\"\" Test on batch \"\"\"\n",
    "        y_pred = self._forward(X)\n",
    "        loss = np.mean(self.loss_function.loss(y, y_pred))\n",
    "        acc = self.loss_function.acc(y, y_pred)\n",
    "        return loss, acc\n",
    "\n",
    "    def fit(self, X, y, n_epochs, batch_size):\n",
    "        \"\"\" Train on n_epochs \"\"\"\n",
    "        for _ in tqdm(range(n_epochs)):\n",
    "            batch_error = []\n",
    "            batch_iterator =  Batcher(X, y, batch_size=batch_size)\n",
    "            for X_batch, y_batch in batch_iterator:\n",
    "                loss, acc = self.train_on_batch(X_batch, y_batch)\n",
    "                batch_error.append(loss)\n",
    "            \n",
    "            # Append training and validation errors\n",
    "            self.errors[\"training\"].append(np.mean(batch_error))\n",
    "            if self.val_set is not None:\n",
    "                val_loss, _ = self.test_on_batch(self.val_set[\"X\"], self.val_set[\"y\"])\n",
    "                self.errors[\"validation\"].append(val_loss)\n",
    "        return self.errors[\"training\"], self.errors[\"validation\"]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict labels of X \"\"\"\n",
    "        return self._forward(X)\n",
    "\n",
    "    def _forward(self, X):\n",
    "        \"\"\" Calculate the output of the NN \"\"\"\n",
    "        layer_output = X\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer.forward(layer_output)\n",
    "        return layer_output\n",
    "\n",
    "    def _backward(self, loss_grad):\n",
    "        \"\"\" Propagate the gradient 'backwards' and update the weights in each layer \"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_grad = layer.backward(loss_grad)\n",
    "\n",
    "    def print_network(self):\n",
    "        print(\"***** Current network *****\")\n",
    "        print('layer', '\\t\\t', 'output_shape', '\\t\\t', 'Input_shape')\n",
    "        for layer in self.layers:\n",
    "            print(layer.layer_name, '\\t\\t', layer.output_shape, '\\t\\t',layer.input_shape)\n",
    "        print('Loss function ', '\\t\\t', self.loss_function.loss_name)\n",
    "        print(\"***************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "543a2fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher(X, y=None, batch_size=64):\n",
    "    \"\"\" batch generator \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    for i in np.arange(0, n_samples, batch_size):\n",
    "        start, end = i, min(i+batch_size, n_samples)\n",
    "        yield X[start:end, ...], y[start:end, ...]\n",
    "\n",
    "class Batcher:\n",
    "    def __init__(self, X, y=None, batch_size=64):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "    def __iter__(self):\n",
    "        return self.batcher(self.X, self.y, self.batch_size)\n",
    "    \n",
    "    def batcher(self, X, y=None, batch_size=64):\n",
    "        \"\"\" batch generator \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        for i in np.arange(0, n_samples, batch_size):\n",
    "            start, end = i, min(i+batch_size, n_samples)\n",
    "            yield X[start:end, ...], y[start:end, ...]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "004cf0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "\n",
    "    def set_input_shape(self, shape):\n",
    "        \"\"\" Sets expected shape for valid forward prop \"\"\"\n",
    "        self.input_shape = shape\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\" Propogates the signal forward in the network \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self, XdA):\n",
    "        \"\"\" Propogates the signal forward in the network \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89f33e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    def __call__(self, Z): # Forward\n",
    "        raise NotImplementedError()\n",
    "    def gradient(self, A): \n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f03d6bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    def loss(self, y_true, y_pred):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def gradient(self, y, y_pred):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def acc(self, y, y_pred):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904130d5",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c4d134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "\n",
    "def relu(Z):\n",
    "    return np.where(Z >= 0, Z, 0)\n",
    "    \n",
    "def softmax(Z):\n",
    "    e_Z = np.exp(Z)\n",
    "    out = e_Z / np.sum(e_Z, axis=1, keepdims=True)\n",
    "    #assert Z.shape == out.shape\n",
    "    return out\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    \"\"\"\n",
    "    Fordward prop & grad\n",
    "    \"\"\"\n",
    "    def __call__(self, Z): # Forward\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    def gradient(self, Z): \n",
    "        return self.__call__(Z) * (1 - self.__call__(Z))\n",
    "\n",
    "class Relu(Activation):\n",
    "    \"\"\"\n",
    "    Fordward prop & grad\n",
    "    \"\"\"\n",
    "    def __call__(self, Z):\n",
    "        return np.where(Z >= 0, Z, 0)\n",
    "    def gradient(self, Z):\n",
    "        return np.where(Z >= 0, 1, 0)\n",
    "\n",
    "\n",
    "class Softmax(Activation):\n",
    "    \"\"\"\n",
    "    Fordward prop & grad\n",
    "    \"\"\"\n",
    "    def __call__(self, Z):\n",
    "        e_Z = np.exp(Z)\n",
    "        out = e_Z / np.sum(e_Z, axis=1, keepdims=True)\n",
    "        #assert Z.shape[0] - 10**(-13) <np.sum(out) < Z.shape[0] + 10**(-13), print(Z, e_Z )\n",
    "        assert Z.shape == out.shape\n",
    "        return out\n",
    "\n",
    "    def gradient(self, Z):\n",
    "        p = self.__call__(Z)\n",
    "        grad = - p[:, :, np.newaxis] *  p[:, np.newaxis, :]\n",
    "        diag = np.arange(p.shape[-1])\n",
    "        grad[:, diag, diag]  = p * (1-p)\n",
    "\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac35943",
   "metadata": {},
   "source": [
    "## Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f73d0892",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "    out_units: (int)\n",
    "    initializer: (string)\n",
    "    lr: (float) learning rate\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, out_units, input_shape=None, initializer = 'normal', lr = 0.06):\n",
    "        self.layer_name ='dense'\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = (out_units,)\n",
    "        self.initializer = initializer\n",
    "        self.lr = lr\n",
    "        self.layer_input = None\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.train = True\n",
    "\n",
    "\n",
    "    def get_output_shape(self):\n",
    "        return self.output_shape\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\" Initialize the weights. Unchanged\n",
    "        \"\"\"\n",
    "        wshape = (self.input_shape[0], self.output_shape[0])\n",
    "        if self.initializer == 'normal':\n",
    "            lim = np.sqrt(6) / math.sqrt(wshape[0]+wshape[1])\n",
    "            self.W = np.random.uniform(-lim, lim, wshape)\n",
    "\n",
    "        if self.initializer == 'ng':\n",
    "            self.W = np.random.randn(wshape[0], wshape[1])/np.sqrt(wshape[0])\n",
    "\n",
    "        self.b = np.zeros(shape = (1, wshape[1]))\n",
    "        assert self.W.shape == (self.input_shape[0], self.output_shape[0])\n",
    "        assert self.b.shape == (1,self.output_shape[0])\n",
    "\n",
    "    def forward(self, A_prev): \n",
    "        self.layer_input = A_prev\n",
    "        self.Z = np.matmul(A_prev, self.W) + self.b\n",
    "        assert self.Z.shape == (A_prev.shape[0], self.W.shape[1])\n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        # Input dZ_prev = dl/dZ_prev\n",
    "        # Output dL/dA = dL/dZ * dZ/dA  = dL/dZ * W^T\n",
    "        W = self.W\n",
    "        A_prev = self.layer_input\n",
    "        norm = A_prev.shape[0]\n",
    "        if self.train:\n",
    "            # Gradiend update dw= dL/dw = dz/dw * dl/dz = A_prev^T dL/dz\n",
    "            # Gradiend update db= dL/bb = dz/db * dl/dz = dL/dz\n",
    "            dW = np.matmul(A_prev.T, dZ) / norm #(2)normalize\n",
    "            db = np.sum(dZ, axis = 0, keepdims = True) #/ norm #(2)normalize\n",
    "            assert dW.shape == W.shape\n",
    "            assert db.shape == self.b.shape\n",
    "            self.W = self.W - self.lr * dW\n",
    "            self.b = self.b - self.lr * db\n",
    "\n",
    "        # Output dL/dA = dL/dZ * dZ/dA  = dL/dZ * W^T\n",
    "        return np.matmul(dZ, W.T) # return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daef878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(Layer):\n",
    "    \"\"\"A layer that flattens a 2D matrix\n",
    "    Parameters:\n",
    "    -----------\n",
    "    name: string\n",
    "        The name of the activation function that will be used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape = None):\n",
    "        self.layer_name = 'flatten'\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def initialize(self):\n",
    "        # Just to set the output shape, but not needed below\n",
    "        coords_to_flatten = 1\n",
    "        for i in self.input_shape:\n",
    "            coords_to_flatten *=i\n",
    "        self.output_shape = (coords_to_flatten,)\n",
    "\n",
    "    def get_output_shape(self):\n",
    "        return self.output_shape\n",
    "\n",
    "    def forward(self, Z):\n",
    "        batch_size= Z.shape[0]\n",
    "        shape = (batch_size, self.output_shape[0])\n",
    "        return Z.reshape(shape)\n",
    "\n",
    "    def backward(self, dA):\n",
    "        batch_size= dA.shape[0]\n",
    "        shape = (batch_size,) + self.input_shape\n",
    "        return dA.reshape(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a1b8ac",
   "metadata": {},
   "source": [
    "## Activation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "243c2bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = {\n",
    "    'sigmoid': Sigmoid,\n",
    "    'relu': Relu\n",
    "}\n",
    "\n",
    "class Activation(Layer):\n",
    "    \"\"\"\n",
    "    A layer that applies an activation operation to the input.\n",
    "        Parameters:\n",
    "    -----------\n",
    "    layer_name: (string) Name of activation layer\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_name):\n",
    "        self.layer_name = layer_name\n",
    "        self.input_shape = None\n",
    "        self.activation_func = activation_functions[layer_name]()\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\" \n",
    "        Set shape\n",
    "        \"\"\"\n",
    "        self.output_shape = self.input_shape\n",
    "\n",
    "    def get_output_shape(self):\n",
    "        return self.output_shape\n",
    "\n",
    "    def forward(self, Z):\n",
    "        self.layer_input = Z\n",
    "        act = self.activation_func(Z)\n",
    "        assert Z.shape == act.shape\n",
    "        return act\n",
    "\n",
    "    def backward(self, dA):\n",
    "        Z = self.layer_input\n",
    "        dact = self.activation_func.gradient(Z)\n",
    "        #assert Z.shape == dact.shape\n",
    "        assert Z.shape == dA.shape\n",
    "        dZ = dact * dA\n",
    "        assert(dZ.shape == (Z.shape))\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb0a4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_SoftMax(Layer):\n",
    "    \"\"\"\n",
    "    A layer that applies an activation operation to the input.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape = None):\n",
    "        self.layer_name = 'softmax'\n",
    "        self.input_shape = input_shape\n",
    "        self.activation_func = Softmax()\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\" \n",
    "        Set shape\n",
    "        \"\"\"\n",
    "        self.output_shape = self.input_shape\n",
    "\n",
    "    def get_output_shape(self):\n",
    "        return self.output_shape\n",
    "\n",
    "    def forward(self, Z):\n",
    "        self.layer_input = Z\n",
    "        act = self.activation_func(Z)\n",
    "        assert Z.shape == act.shape\n",
    "        return act\n",
    "\n",
    "    def backward(self, dA):\n",
    "        Z = self.layer_input\n",
    "        dact = self.activation_func.gradient(Z)\n",
    "        #assert Z.shape == dact.shape\n",
    "        assert Z.shape == dA.shape\n",
    "        dZ = np.sum(np.multiply(dact, dA[:, np.newaxis,:]), axis = 2)\n",
    "        assert(dZ.shape == (Z.shape))\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f78f37e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(Loss):\n",
    "    def __init__(self):\n",
    "        self.loss_name = 'CrossEntropy'\n",
    "\n",
    "    def loss(self, y, AL):\n",
    "        # Avoid division by zero\n",
    "        AL = np.clip(AL, 1e-15, 1 - 1e-15)\n",
    "        return (- y * np.log(AL) - (1 - y) * np.log(1 - AL)) #/(y.shape[-1])#(1)normalize in main\n",
    "\n",
    "    def acc(self, y, AL):\n",
    "        y, AL = y[0], AL[0]\n",
    "        return np.sum(y == AL) /len(y)\n",
    "\n",
    "    def gradient(self, y, AL):\n",
    "        # Avoid division by zero\n",
    "        AL = np.clip(AL, 1e-15, 1 - 1e-15)\n",
    "        #print  ((- (y / AL) + (1 - y) / (1 - AL) ).shape,'cross-function output dA')\n",
    "        return (- (y / AL) + (1 - y) / (1 - AL)) #/(y.shape[-1])#(2)normalize when updating grad\n",
    "\n",
    "\n",
    "class MultiClassCrossEntropy(Loss):\n",
    "    def __init__(self):\n",
    "        self.loss_name = 'MultiClassCrossEntropy'\n",
    "\n",
    "    def loss(self, y, AL):\n",
    "        #assert AL.shape[0] - 10**(-13) <np.abs(np.sum(AL)) < AL.shape[0] + 10**(-13), print(AL, 1)\n",
    "        return -np.sum(y * np.log(AL), axis= 1, keepdims = True) #/(y.shape[-1])#(1)normalize in main\n",
    "    \n",
    "    def acc(self, y_true, AL):\n",
    "        preds = np.array([y for y in np.argmax(AL, axis=1)]).squeeze()\n",
    "        y_true = np.array([y for y in np.argmax(y_true, axis=1)]).squeeze()\n",
    "        return np.mean(preds == y_true)\n",
    "\n",
    "    def gradient(self, y, AL):\n",
    "        # Avoid division by zero\n",
    "        AL = np.clip(AL, 1e-15, 1 - 1e-15)\n",
    "\n",
    "        assert(AL.shape == (-(y / AL)).shape)\n",
    "\n",
    "        return - (y / AL) # /(y.shape[-1])#(2)normalize when updating grad\n",
    "\n",
    "class SoftmaxCrossEntropy(Loss):\n",
    "    def __init__(self):\n",
    "        self.loss_name = 'SoftmaxCrossEntropy'\n",
    "\n",
    "    def loss(self, y, Z):\n",
    "        # Avoid division by zero\n",
    "        Z_aux = Z.shape\n",
    "        Z = Z - np.max(Z, axis = 1, keepdims=True)\n",
    "        assert Z_aux ==Z.shape\n",
    "\n",
    "        log_e_Z = Z- np.log(np.sum( np.exp(Z), axis=1, keepdims=True))\n",
    "        return (-np.sum( y * log_e_Z ,axis= 1))  # (1)normalize in main\n",
    "    \n",
    "\n",
    "    def acc(self, y, Z):\n",
    "        AL = softmax(Z)\n",
    "        preds = np.array([y for y in np.argmax(AL, axis=1)]).squeeze()\n",
    "        y = np.array([y for y in np.argmax(y, axis=1)]).squeeze()\n",
    "        return np.mean(preds == y)\n",
    "\n",
    "\n",
    "    def gradient(self, y, Z):\n",
    "        Z -= np.max(Z, axis = 1, keepdims=True)\n",
    "        p = np.exp(Z)/ np.sum( np.exp(Z), axis=1, keepdims=True)\n",
    "\n",
    "        # Avoid division by zero\n",
    "        return  (-y + p)  #(2)Normalize when updating grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db40b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "\n",
    "def vectorized_result(y):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[y] = 1.0\n",
    "    return e\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "def load_dataset(flatten=False, unsqueeze = False):\n",
    "    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # normalize x\n",
    "    X_train = X_train.astype(float) / 255.\n",
    "    X_test = X_test.astype(float) / 255.\n",
    "\n",
    "    y_train = np.array([vectorized_result(y) for y in y_train]).squeeze()\n",
    "    y_test = np.array([vectorized_result(y) for y in y_test]).squeeze()\n",
    "\n",
    "    # we reserve the last 10000 training examples for validation\n",
    "    X_train, X_val = X_train[:-10000,...], X_train[-10000:,...]\n",
    "    y_train, y_val = y_train[:-10000,...], y_train[-10000:,...]\n",
    "\n",
    "    if flatten:\n",
    "        X_train = X_train.reshape((X_train.shape[0], 28*28))\n",
    "        X_val = X_val.reshape((X_test.shape[0], 28*28))\n",
    "        X_test = X_test.reshape((X_val.shape[0], 28*28))\n",
    "\n",
    "    if unsqueeze:\n",
    "        X_train = X_train[:,np.newaxis, :,:]\n",
    "        X_val = X_val[:,np.newaxis, :,:]\n",
    "        X_test = X_test[:,np.newaxis, :,:]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "train_x, train_y, val_x, val_y, test_x, test_y = load_dataset(False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "889b4f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClassCrossEntropy\n",
      "***** Current network *****\n",
      "layer \t\t output_shape \t\t Input_shape\n",
      "flatten \t\t (784,) \t\t (1, 28, 28)\n",
      "dense \t\t (100,) \t\t (784,)\n",
      "relu \t\t (100,) \t\t (100,)\n",
      "dense \t\t (200,) \t\t (100,)\n",
      "relu \t\t (200,) \t\t (200,)\n",
      "dense \t\t (10,) \t\t (200,)\n",
      "softmax \t\t (10,) \t\t (10,)\n",
      "Loss function  \t\t MultiClassCrossEntropy\n",
      "***************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 5/5 [00:10<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuarecy: 0.96326\n",
      "Test accuarecy: 0.9543\n",
      "Training loss: 0.08606785005410234\n"
     ]
    }
   ],
   "source": [
    "# Building network\n",
    "\n",
    "print('MultiClassCrossEntropy')\n",
    "md=NeuralNetwork(MultiClassCrossEntropy)\n",
    "np.random.seed(1)\n",
    "\n",
    "n_x = 28*28    # num_px * num_px * 3\n",
    "lr = 0.05    # num_px * num_px * 3\n",
    "md.add(Flatten(input_shape = (1, 28, 28)))\n",
    "md.add(Dense(100, initializer = 'normal', lr = lr))\n",
    "md.add(Activation('relu'))\n",
    "md.add(Dense(200, initializer = 'normal', lr = lr))\n",
    "md.add(Activation('relu'))\n",
    "md.add(Dense(10, initializer = 'normal', lr = lr))\n",
    "md.add(Activation_SoftMax())\n",
    "\n",
    "md.print_network()\n",
    "\n",
    "#train\n",
    "hist = md.fit(train_x, train_y, n_epochs=5, batch_size=32)\n",
    "\n",
    "\n",
    "def accuracy(test_x, test_y):\n",
    "    preds = md.predict(test_x)\n",
    "    preds = np.array([y for y in np.argmax(preds, axis=1)]).squeeze()\n",
    "    test_y_ = np.array([y for y in np.argmax(test_y, axis=1)]).squeeze()\n",
    "    return np.mean(preds == test_y_)\n",
    "\n",
    "## Evalaution\n",
    "print('Training accuarecy: {}'.format(accuracy(train_x , train_y)))\n",
    "print('Test accuarecy: {}'.format(accuracy(test_x , test_y)))\n",
    "print('Training loss: {}'.format(hist[0][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fddd7ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftmaxCrossEntropy\n",
      "***** Current network *****\n",
      "layer \t\t output_shape \t\t Input_shape\n",
      "flatten \t\t (784,) \t\t (28, 28, 1)\n",
      "dense \t\t (100,) \t\t (784,)\n",
      "relu \t\t (100,) \t\t (100,)\n",
      "dense \t\t (200,) \t\t (100,)\n",
      "relu \t\t (200,) \t\t (200,)\n",
      "dense \t\t (10,) \t\t (200,)\n",
      "Loss function  \t\t SoftmaxCrossEntropy\n",
      "***************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 5/5 [00:12<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuarecy: 0.96326\n",
      "Test accuarecy: 0.9543\n",
      "Training loss: 0.08606785005410234\n"
     ]
    }
   ],
   "source": [
    "print('SoftmaxCrossEntropy')\n",
    "md=NeuralNetwork(SoftmaxCrossEntropy)\n",
    "np.random.seed(1)\n",
    "lr = 0.05\n",
    "n_x = 784    \n",
    "md.add(Flatten(input_shape = (28, 28, 1,)))\n",
    "md.add(Dense(100, initializer = 'normal', lr = lr))\n",
    "md.add(Activation('relu'))\n",
    "md.add(Dense(200, initializer = 'normal', lr = lr))\n",
    "md.add(Activation('relu'))\n",
    "md.add(Dense(10, initializer = 'normal', lr = lr))\n",
    "\n",
    "md.print_network()\n",
    "\n",
    "#train\n",
    "hist = md.fit(train_x, train_y, n_epochs=5, batch_size=32)\n",
    "\n",
    "def accuracy(test_x, test_y):\n",
    "    preds = md.predict(test_x)\n",
    "    preds = np.array([y for y in np.argmax(preds, axis=1)]).squeeze()\n",
    "    test_y_ = np.array([y for y in np.argmax(test_y, axis=1)]).squeeze()\n",
    "    return np.mean(preds == test_y_)\n",
    "\n",
    "## Evalaution\n",
    "print('Training accuarecy: {}'.format(accuracy(train_x , train_y)))\n",
    "print('Test accuarecy: {}'.format(accuracy(test_x , test_y)))\n",
    "print('Training loss: {}'.format(hist[0][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "868d7f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropy\n",
      "***** Current network *****\n",
      "layer \t\t output_shape \t\t Input_shape\n",
      "flatten \t\t (784,) \t\t (1, 28, 28)\n",
      "dense \t\t (100,) \t\t (784,)\n",
      "relu \t\t (100,) \t\t (100,)\n",
      "dense \t\t (200,) \t\t (100,)\n",
      "relu \t\t (200,) \t\t (200,)\n",
      "dense \t\t (10,) \t\t (200,)\n",
      "sigmoid \t\t (10,) \t\t (10,)\n",
      "Loss function  \t\t CrossEntropy\n",
      "***************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 5/5 [00:11<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuarecy: 0.97302\n",
      "Test accuarecy: 0.9635\n",
      "Training loss: 0.01668239152653071\n"
     ]
    }
   ],
   "source": [
    "print('CrossEntropy')\n",
    "md=NeuralNetwork(CrossEntropy)\n",
    "np.random.seed(1)\n",
    "\n",
    "n_x = 28*28    # num_px * num_px * 3\n",
    "lr = 0.05    # num_px * num_px * 3\n",
    "md.add(Flatten(input_shape = (1, 28, 28)))\n",
    "md.add(Dense(100, initializer = 'normal', lr = lr))\n",
    "md.add(Activation('relu'))\n",
    "md.add(Dense(200, initializer = 'normal', lr = lr))\n",
    "md.add(Activation('relu'))\n",
    "md.add(Dense(10, initializer = 'normal', lr = lr))\n",
    "md.add(Activation('sigmoid'))\n",
    "\n",
    "md.print_network()\n",
    "\n",
    "#train\n",
    "hist = md.fit(train_x, train_y, n_epochs=5, batch_size=32)\n",
    "\n",
    "\n",
    "def accuracy(test_x, test_y):\n",
    "    preds = md.predict(test_x)\n",
    "    preds = np.array([y for y in np.argmax(preds, axis=1)]).squeeze()\n",
    "    test_y_ = np.array([y for y in np.argmax(test_y, axis=1)]).squeeze()\n",
    "    return np.mean(preds == test_y_)\n",
    "\n",
    "## Evalaution\n",
    "print('Training accuarecy: {}'.format(accuracy(train_x , train_y)))\n",
    "print('Test accuarecy: {}'.format(accuracy(test_x , test_y)))\n",
    "print('Training loss: {}'.format(hist[0][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c16b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snakes_kernel",
   "language": "python",
   "name": "snakes_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
